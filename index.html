---
layout: default
title: Home
---



<div class="container">

    
  <h2 class="page-title">News </h2>  
  <div>
    <ul>
      <li> <p class="content-home"> [Apr 2019] <a href="http://cvpr2019.thecvf.com/"> Our FERAtt project is accepted to Workshop CVPR 2019!!!</a> </p> </li>
      <li> <p class="content-home"> [Dec 2018] <a href="https://doi.org/10.1016/j.imavis.2018.11.001"> Our colorcheck paper is accepted to IMAVIS!!!</a> </p> </li>
    </ul>
  </div>

  <h2 class="page-title">Publications </h2>  
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"> 
    


    <!-- item 5 -->
   

    <!-- item 4 -->
    <tr>         
      <td width="20%" valign="top">
        <!-- image -->
        <div class="col-sm-2 project-thumbnail-home"></div>
        <img src="public/img/burst.png" class="img-responsive-home" alt="ContextLocNet">
        </div>

        <!-- description -->
        <td width="80%" valign="top">
          <div class="project-title-home">
          <strong>Burst ranking for blind multi-image deblurring</strong>
          </div>          
          <div class="project-description-home">
              Fidel A. Guerrero Peña, Pedro D. Marrero Fernández, Tsang Ing Ren, Jorge JG and Nishihara, Ricardo; <em> - </em> 2019.
          </div>
          <div class="project-links-home" id="attfer_bib" >            
            [<a href="paper.html">paper</a>] &nbsp;
            [<a href="https://arxiv.org/abs/1810.12121">arxiv</a>] &nbsp;
            [<a href="https://github.com/pedrodiamel/">code</a>] &nbsp;
            [<a href="javascript:toggleblock('attfer_abs')">abstract</a>] &nbsp;
            [<a href="javascript:togglebib('attfer_bib')" shape="rect" class="togglebib">bibtex</a>] &nbsp;

            <div class="bibtex-home"  >
              <p align="justify" > <i id="attfer_abs" style="display: none;"> We propose a new incremental aggregation algorithm for multi-image deblurring with automatic image selection. The primary motivation is that current bursts deblurring methods do not handle well situations in which misalignment or out-of-context frames are present in the burst. These real-life situations result in poor reconstructions or manual selection of the images that will be used to deblur. Automatically selecting best frames within the burst to improve the base reconstruction is challenging because the amount of possible images fusions is equal to the power set cardinal. Here, we approach the multi-image deblurring problem as a two steps process. First, we successfully learn a comparison function to rank a burst of images using a deep convolutional neural network. Then, an incremental Fourier burst accumulation with a reconstruction degradation mechanism is applied fusing only less blurred images that are sufficient to maximize the reconstruction quality. Experiments with the proposed algorithm have shown superior results when compared to other similar approaches, outperforming other methods described in the literature in previously described situations. We validate our findings on several synthetic and real datasets. </i></p>
              <pre style="display: none;">
@article{pena2018burst,
  title={Burst ranking for blind multi-image deblurring},
  author={Peña, Fidel A Guerrero and Fernández, Pedro D Marrero 
          and Ren, Tsang Ing and Leandro, Jorge JG and Nishihara, Ricardo},
  journal={arXiv preprint arXiv:1810.12121},
  year={2018}
}
              </pre> 
            </div>
          </div>
        </td> 
      </tr>
      </td>
    </tr>




    <!-- item 3  -->

    <tr>         
      <td width="20%" valign="top">
        <!-- image -->
        <div class="col-sm-2 project-thumbnail-home"></div>
        <img src="https://github.com/pedrodiamel/ferattention/raw/master/rec/emotion.gif" class="img-responsive-home" alt="ContextLocNet">
        </div>

        <!-- description -->
        <td width="80%" valign="top">
          <div class="project-title-home">
          <strong>FERAtt: Facial Expression Recognition with Attention Net</strong>
          </div>          
          <div class="project-description-home">
            Pedro D. Marrero Fernández, Fidel A. Guerrero Peña, Tsang Ing Ren, Alexandre Cunha; <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops </em> 2019.
          </div>
          <div class="project-links-home" id="attfer_bib" >            
            [<a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/MBCCV/Fernandez_FERAtt_Facial_Expression_Recognition_With_Attention_Net_CVPRW_2019_paper.html">paper</a>] &nbsp;
            [<a href="https://arxiv.org/abs/1902.03284">arxiv</a>] &nbsp;
            [<a href="https://github.com/pedrodiamel/ferattention">code</a>] &nbsp;
            [<a href="javascript:toggleblock('attfer_abs')">abstract</a>] &nbsp;
            [<a href="javascript:togglebib('attfer_bib')" shape="rect" class="togglebib">bibtex</a>] &nbsp;

            <div class="bibtex-home"  >
              <p align="justify" > <i id="attfer_abs" style="display: none;"> We present a new end-to-end network architecture for facial expression recognition with an attention model. It focuses attention in the human face and uses a Gaussian space representation for expression recognition. We devise this architecture based on two fundamental complementary components: (1) facial image correction and attention and (2) facial expression representation and classification. The first component uses an encoder-decoder style network and a convolutional feature extractor that are pixel-wise multiplied to obtain a feature attention map. The second component is responsible for obtaining an embedded representation and classification of the facial expression. We propose a loss function that creates a Gaussian structure on the representation space. To demonstrate the proposed method, we create two larger and more comprehensive synthetic datasets using the traditional BU3DFE and CK+ facial datasets. We compared results with the PreActResNet18 baseline. Our experiments on these datasets have shown the superiority of our approach in recognizing facial expressions. </i></p>
              <pre style="display: none;">
InProceedings{Fernandez_2019_CVPR_Workshops,
  author = {Marrero Fernandez, Pedro D. and Guerrero Pena, Fidel A. 
            and Ing Ren, Tsang and Cunha, Alexandre},
  title = {FERAtt: Facial Expression Recognition With Attention Net},
  booktitle = {The IEEE Conference on Computer Vision 
            and Pattern Recognition (CVPR) Workshops},
  month = {June},
  year = {2019}
  }
              </pre> 
            </div>
          </div>
        </td> 
      </tr>
      </td>
    </tr>



    <!-- item 2 -->
    <tr>         
      <td width="20%" valign="top">
        <!-- image -->
        <div class="col-sm-2 project-thumbnail-home"></div>
        <img src="https://github.com/pedrodiamel/colorchecker-detection/raw/master/rec/mcc.gif" class="img-responsive-home" alt="ContextLocNet">
        </div>

        <!-- description -->
        <td width="80%" valign="top">
          <div class="project-title-home">
          <strong>Fast and Robust Multiple ColorChecker Detection using Deep Convolutional Neural Networks</strong>
          </div>          
          <div class="project-description-home">
            Pedro D. Marrero Fernández, Fidel A. Guerrero Peña, Tsang Ing Ren, Jorge J.G. Leandro; <em>Image and Vision Computing</em> 2019.
          </div>
          <div class="project-links-home" id="colorchecker_detection" >            
            [<a href="https://doi.org/10.1016/j.imavis.2018.11.001">paper</a>] &nbsp;
            [<a href="https://arxiv.org/abs/1810.08639" >arxiv</a>] &nbsp;
            [<a href="https://github.com/pedrodiamel/colorchecker-detection">code</a>] &nbsp;
            [<a href="javascript:toggleblock('colorchecker_detection_abs')">abstract</a>] &nbsp;
            [<a href="javascript:togglebib('colorchecker_detection')" shape="rect" class="togglebib">bibtex</a>] &nbsp;

            <div class="bibtex-home"  >
              <p align="justify" > <i id="colorchecker_detection_abs" style="display: none;"> ColorCheckers are reference standards that professional photographers and filmmakers use to ensure predictable results under every lighting condition. The objective of this work is to propose a new fast and robust method for automatic ColorChecker detection. The process is divided into two steps: (1) ColorCheckers localization and (2) ColorChecker patches recognition. For the ColorChecker localization, we trained a detection convolutional neural network using synthetic images. The synthetic images are created with the 3D models of the ColorChecker and different background images. The output of the neural networks are the bounding box of each possible ColorChecker candidates in the input image. Each bounding box defines a cropped image which is evaluated by a recognition system, and each image is canonized with regards to color and dimensions. Subsequently, all possible color patches are extracted and grouped with respect to the center's distance. Each group is evaluated as a candidate for a ColorChecker part, and its position in the scene is estimated. Finally, a cost function is applied to evaluate the accuracy of the estimation. The method is tested using real and synthetic images. The proposed method is fast, robust to overlaps and invariant to affine projections. The algorithm also performs well in case of multiple ColorCheckers detection. </i></p>
              <pre style="display: none;">
@article{MARREROFERNANDEZ2018,
    title = "Fast and Robust Multiple ColorChecker Detection 
             using Deep Convolutional Neural Networks",
    journal = "Image and Vision Computing",
    year = "2018",
    issn = "0262-8856",
    author = "Pedro D. Marrero Fernández and Fidel A. Guerrero Peña 
              and Tsang Ing Ren and Jorge J.G. Leandro",
  }
              </pre> 
            </div>
          </div>
        </td> 
      </tr>
      </td>
    </tr>
      

    <!-- item 1 -->

    <tr>         
      <td width="20%" valign="top">
        <!-- image -->
        <div class="col-sm-2 project-thumbnail-home"></div>
        <img src="public/img/mwlcs.png" class="img-responsive-home" alt="ContextLocNet">
        </div>

        <!-- description -->
        <td width="80%" valign="top">
          <div class="project-title-home">
          <strong>Multiclass Weighted Loss for Instance Segmentation of Cluttered Cells</strong>
          </div>          
          <div class="project-description-home">
              Fidel A Guerrero-Pena, Pedro D Marrero Fernandez, Tsang Ing Ren, Mary Yui, Ellen Rothenberg, Alexandre Cunha; <em>IEEE International Conference on Image Processing</em> 2018.
          </div>
          <div class="project-links-home" id="mwlis" >            
            [<a href="https://doi.org/10.1109/ICIP.2018.8451187">paper</a>] &nbsp;
            [<a href="https://arxiv.org/abs/1802.07465" >arxiv</a>] &nbsp;
            [<a href=" ">code</a>] &nbsp;
            [<a href="javascript:toggleblock('mwlis_abs')">abstract</a>] &nbsp;
            [<a href="javascript:togglebib('mwlis')" shape="rect" class="togglebib">bibtex</a>] &nbsp;

            <div class="bibtex-home"  >
              <p align="justify" > <i id="mwlis_abs" style="display: none;"> We propose a new multiclass weighted loss function for instance segmentation of cluttered cells. We are primarily motivated by the need of developmental biologists to quantify and model the behavior of blood T–cells which might help us in understanding their regulation mechanisms and ultimately help researchers in their quest for developing an effective immunotherapy cancer treatment. Segmenting individual touching cells in cluttered regions is challenging as the feature distribution on shared borders and cell foreground are similar thus difficulting discriminating pixels into proper classes. We present two novel weight maps applied to the weighted cross entropy loss function which take into account both class imbalance and cell geometry. Binary ground truth training data is augmented so the learning model can handle not only foreground and background but also a third touching class. This framework allows training using U-Net. Experiments with our formulations have shown superior results when compared to other similar schemes, outperforming binary class models with significant improvement of boundary adequacy and instance detection. We validate our results on manually annotated microscope images of T–cells. </i></p>
              <pre style="display: none;">
@inproceedings{guerrero2018multiclass,
  title={Multiclass weighted loss for instance segmentation 
    of cluttered cells},
  author={Guerrero-Pena, Fidel A and Fernandez, Pedro D Marrero and 
    Ren, Tsang Ing and Yui, Mary and Rothenberg, Ellen and Cunha, Alexandre},
  booktitle={2018 25th IEEE International Conference on 
    Image Processing (ICIP)},
  pages={2451--2455},
  year={2018},
  organization={IEEE}
}
              </pre> 
            </div>
          </div>
        </td> 
      </tr>
      </td>
    </tr>
    


    <!-- item 0 -->




  </table>



  <h2 class="page-title">Abaut Me </h2> 
  <p>My name is Pedro Diamel Marrero Fernandez, I'm Computer Science PhD student at the <a href="http://www.cin.ufpe.br">Informatics Center (CIn)</a>, <a href="http://www.ufpe.br">Federal University of Pernambuco</a>. I'm passionate about deep learning with a strong focus on Computer Vision and Image Processing. Currently, I'm an artificial intelligence researcher at Motorola LLC - CIn partnership Lab. </p>


  

</div>
