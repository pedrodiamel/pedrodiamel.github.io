<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <meta name="description" content="My personal web site :)">
    <meta name="author" content="Pedro D. Marrero">

    <title>Pedro D. Marrero</title>

    <meta name="msapplication-TileColor" content="#101802">
    <meta name="theme-color" content="#101802">

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@100;200;300;400&display=swap"
        rel="stylesheet" />

    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />

    <link rel="stylesheet" href="css/styles.css" />



</head>

<body>


    <div class="page-home">

        <!-- HEADER -->
        <header>
        </header>

        <main>

            <div class="main-container">

                <!-- HERO -->
                <div class="main-hero">

                    <div class="main-logo">


                        <div id="main-image">
                            <img class="main-image" src="assets/me.png" alt="me" />
                        </div>
                        <div id="main-descripion" class="main-description">
                            <h1 class="main-description-name">
                                Pedro D. Marrero
                            </h1>
                            <h2 class="main-description-complement">
                                I'm ^^) training neural networks !!! ðŸ§ ðŸ¤–ðŸ’¥
                            </h2>
                            <div class="main-description-list-icon">
                                <a href="https://scholar.google.com.br/citations?hl=en&user=g7Vcpe4AAAAJ"> <img
                                        src="assets/gs.png" class="main-description-icon" /> </a>
                                <a href="https://twitter.com/Pedrodiamel"> <img src="assets/ctwitter.svg"
                                        class="main-description-icon" /> </a>
                                <a href="https://github.com/pedrodiamel"> <img src="assets/cgithub.svg"
                                        class="main-description-icon" /> </a>
                                <!-- <a href="#"> <img src="assets/crss.svg" class="main-description-icon" /> </a> -->
                                <a href="mailto:pedrodiamel@gmail.com"> <img src="assets/cemail.svg"
                                        class="main-description-icon" /> </a>
                            </div>
                        </div>

                    </div>

                    <div class="main-function">
                        <canvas id="canvas-function" class="main-function-canva"></canvas>
                    </div>


                    <div id="main-next" class="">
                        <span class="material-symbols-outlined main-next-button"> expand_more </span>
                    </div>

                </div>


                <!-- CONTENT -->
                <div class="main-content">

                    <section class="setion-content">
                        <div class="content-space"></div>
                        <div class="">
                            <h1 class="content-container">
                                <span class="material-symbols-outlined content-span"> Info </span>
                                About
                            </h1><br>
                            <p class="">Pedro is a distinguished Senior Technical Software Manager at the Recife Center
                                for Advanced Studies and Systems (C.E.S.A.R) in Brazil. His role primarily entails
                                leading and overseeing applied research initiatives focusing on cutting-edge
                                technologies, including Computer Vision, Machine Learning, and Deep Learning.
                            </p>
                        </div>
                        <div id="team" class="setion-content-team">
                            <div class="" id="executives">
                                <h2 class="setion-content-team-executives">
                                    Experience
                                </h2><br>
                                <div class="setion-content-team-description">

                                    <div class="setion-row">
                                        <div class="setion-timespan">
                                            2023-
                                        </div>
                                        <div class="setion-ico">
                                            <div class="setion-entry-dot"></div>
                                            <img src="assets/cesar.png" />
                                        </div>

                                        <div class="setion-desc">
                                            <h3 class="font-medium">Senior Technical Software Manager</h3>
                                            <p class="setion-content-team-text"> My role primarily entails leading and
                                                overseeing applied research initiatives focusing on cutting-edge
                                                technologies, including Computer Vision, Machine Learning, and Deep
                                                Learning.</p>
                                        </div>
                                    </div>

                                    <div class="setion-row">
                                        <div class="setion-timespan">
                                            2019-2021
                                        </div>
                                        <div class="setion-ico">
                                            <div class="setion-entry-dot"></div>
                                            <img src="assets/acceture.png" />
                                        </div>

                                        <div class="setion-desc">
                                            <h3 class="font-medium">Software Engineer Consultant</h3>
                                            <p class="setion-content-team-text"> Liquid Studios, part of Accenture
                                                Technology Innovation, Brazil.</p>
                                        </div>
                                    </div>

                                    <div class="setion-row">
                                        <div class="setion-timespan">
                                            2015-2019
                                        </div>
                                        <div class="setion-ico">
                                            <div class="setion-entry-dot"></div>
                                            <img src="assets/cin.png" />
                                        </div>

                                        <div class="setion-desc">
                                            <h3 class="font-medium">Senior Research</h3>
                                            <p class="setion-content-team-text"> CIn/Motorola Partnership. Centro de
                                                InformÃ¡tica, Universidade Federal de Pernambuco, Brazil.</p>
                                        </div>
                                    </div>


                                    <div class="setion-row">
                                        <div class="setion-timespan">
                                            2010-2015
                                        </div>
                                        <div class="setion-ico">
                                            <div class="setion-entry-dot"></div>
                                            <img src="assets/uo.png" />
                                        </div>

                                        <div class="setion-desc">
                                            <h3 class="font-medium">Lecturer in Computer Science</h3>
                                            <p class="setion-content-team-text"> Universidad de Oriente, Santiago de
                                                Cuba, Cuba.</p>
                                        </div>
                                    </div>

                                </div>
                            </div>
                        </div>

                    </section>

                    <p class="section-split">
                        ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                    </p>

                    <!-- Research -->

                    <section id="" class="setion-content">
                        <div id="contact" class="content-space"></div>
                        <div class="">
                            <h1 class="content-container">
                                <span class="material-symbols-outlined content-span"> experiment </span>
                                Research
                            </h1><br>
                        </div>


                        <table class="research-table">

                            <!-- item 7 -->

                            <tr>
                                <td class="research-table-image-td">
                                    <img src="assets/papers/dml.gif" class="research-table-image" alt="ContextLocNet">
                                </td>

                                <!-- description -->
                                <td class="research-table-desc-td">
                                    <h3 class="font-medium"> Deep Metric Structured Learning For Facial Expression
                                        Recognition
                                    </h3>
                                    <div class="research-table-desc-authors">
                                        Pedro D. Marrero Fernandez, Tsang Ing Ren, Tsang Ing Jyh, Fidel A. Guerrero
                                        PeÃ±a, Alexandre
                                        Cunha; <em> - </em> 2020.
                                    </div>
                                    <div class="project-links-home" id="dml_bib">
                                        [<a href="paper.html">paper</a>] &nbsp;
                                        [<a href="https://arxiv.org/abs/2001.06612">arxiv</a>] &nbsp;
                                        [<a href="https://github.com/pedrodiamel/">code</a>] &nbsp;
                                        [<a href="javascript:toggleblock('dml_abs')">abstract</a>] &nbsp;
                                        [<a href="javascript:togglebib('dml_bib')" shape="rect"
                                            class="togglebib">bibtex</a>] &nbsp;

                                        <div class="research-table-desc-abstract">
                                            <p align="justify"> <i id="dml_abs" style="display: none;"> We propose a
                                                    deep metric
                                                    learning model to create embedded sub-spaces with a well defined
                                                    structure. A
                                                    new
                                                    loss function that imposes Gaussian structures on the output space
                                                    is introduced
                                                    to
                                                    create these sub-spaces thus shaping the distribution of the data.
                                                    Having a
                                                    mixture
                                                    of Gaussians solution space is advantageous given its simplified and
                                                    well
                                                    established structure. It allows fast discovering of classes within
                                                    classes and
                                                    the
                                                    identification of mean representatives at the centroids of
                                                    individual classes.
                                                    We
                                                    also propose a new semi-supervised method to create sub-classes. We
                                                    illustrate
                                                    our
                                                    methods on the facial expression recognition problem and validate
                                                    results on the
                                                    FER+, AffectNet, Extended Cohn-Kanade (CK+), BU-3DFE, and JAFFE
                                                    datasets. We
                                                    experimentally demonstrate that the learned embedding can be
                                                    successfully used
                                                    for
                                                    various applications including expression retrieval and emotion
                                                    recognition.
                                                </i>
                                            </p>
                                            <pre style="display: none;">
  @article{marrero2020deep,
    title={Deep Metric Structured Learning For Facial Expression Recognition},
    author={Marrero Fernandez, Pedro D and Ren, Tsang Ing and Jyh, Tsang Ing
      and Guerrero Pe{\~n}a, Fidel A and Cunha, Alexandre},
    journal={arXiv},
    pages={arXiv--2001},
    year={2020}
  }
                </pre>
                                        </div>
                                    </div>
                                </td>
                            </tr>
                            </td>
                            </tr>

                            <!-- item 6 -->

                            <tr>
                                <td class="research-table-image-td">
                                    <img src="assets/papers/js.png" class="research-table-image" alt="ContextLocNet">
                                </td>

                                <!-- description -->
                                <td class="research-table-desc-td">
                                    <h3 class="font-medium"> J Regularization Improves Imbalanced Multiclass
                                        Segmentation
                                    </h3>
                                    <div class="research-table-desc-authors">
                                        Fidel A. Guerrero PeÃ±a, Pedro D. Marrero Fernandez, Paul T. Tarr, Tsang Ing Ren,
                                        Elliot M.
                                        Meyerowitz, Alexandre Cunha; <em> - </em> 2019.
                                    </div>
                                    <div class="project-links-home" id="js_bib">
                                        [<a href="paper.html">paper</a>] &nbsp;
                                        [<a href="https://arxiv.org/abs/1910.09783">arxiv</a>] &nbsp;
                                        [<a href="https://github.com/pedrodiamel/">code</a>] &nbsp;
                                        [<a href="https://youtu.be/8jp9N45tV0Y">video</a>] &nbsp;
                                        [<a href="javascript:toggleblock('js_abs')">abstract</a>] &nbsp;
                                        [<a href="javascript:togglebib('js_bib')" shape="rect"
                                            class="togglebib">bibtex</a>] &nbsp;

                                        <div class="research-table-desc-abstract">
                                            <p align="justify"> <i id="js_abs" style="display: none;"> We propose a new
                                                    loss formulation
                                                    to further advance the multiclass segmentation of cluttered cells
                                                    under weakly
                                                    supervised conditions. We improve the separation of touching and
                                                    immediate cells,
                                                    obtaining sharp segmentation boundaries with high adequacy, when we
                                                    add Youden's J
                                                    statistic regularization term to the cross entropy loss. This
                                                    regularization
                                                    intrinsically supports class imbalance thus eliminating the
                                                    necessity of explicitly
                                                    using weights to balance training. Simulations demonstrate this
                                                    capability and show
                                                    how the regularization leads to better results by helping advancing
                                                    the optimization
                                                    when cross entropy stalls. We build upon our previous work on
                                                    multiclass
                                                    segmentation by adding yet another training class representing gaps
                                                    between adjacent
                                                    cells. This addition helps the classifier identify narrow gaps as
                                                    background and no
                                                    longer as touching regions. We present results of our methods for 2D
                                                    and 3D images,
                                                    from bright field to confocal stacks containing different types of
                                                    cells, and we
                                                    show that they accurately segment individual cells after training
                                                    with a limited
                                                    number of annotated images, some of which are poorly annotated. </i>
                                            </p>
                                            <pre style="display: none;">
@article{pena2019j,
title={J Regularization Improves Imbalanced Multiclass Segmentation},
author={Pe{\~n}a, Fidel A Guerrero and Fernandez, Pedro D Marrero and
Tarr, Paul T and Ren, Tsang Ing and Meyerowitz, Elliot M and Cunha, Alexandre},
journal={arXiv preprint arXiv:1910.09783},
year={2019}
}
            </pre>
                                        </div>
                                    </div>
                                </td>
                            </tr>
                            </td>
                            </tr>

                            <!-- item 5 -->

                            <tr>
                                <td class="research-table-image-td">
                                    <img src="assets/papers/ws.png" class="research-table-image" alt="ContextLocNet">
                                </td>

                                <!-- description -->
                                <td class="research-table-desc-td">
                                    <h3 class="font-medium"> A Weakly Supervised Method for Instance Segmentation of
                                        Biological Cells
                                    </h3>
                                    <div class="research-table-desc-authors">
                                        Fidel A. Guerrero-PeÃ±a, Pedro D. Marrero Fernandez, Tsang Ing Ren, Alexandre
                                        Cunha; <em> Domain Adaptation and Representation Transfer and Medical Image
                                            Learning with Less Labels and Imperfect Data. Springer, Cham </em> 2019.
                                    </div>
                                    <div class="project-links-home" id="wsm_bib">
                                        [<a
                                            href="https://link.springer.com/chapter/10.1007/978-3-030-33391-1_25">paper</a>]
                                        &nbsp;
                                        [<a href="https://arxiv.org/abs/1908.09891">arxiv</a>] &nbsp;
                                        [<a href="https://github.com/pedrodiamel/">code</a>] &nbsp;
                                        [<a href="javascript:toggleblock('wsm_abs')">abstract</a>] &nbsp;
                                        [<a href="javascript:togglebib('wsm_bib')" shape="rect"
                                            class="togglebib">bibtex</a>] &nbsp;

                                        <div class="research-table-desc-abstract">
                                            <p align="justify"> <i id="wsm_abs" style="display: none;"> We present a
                                                    weakly supervised
                                                    deep learning method to perform instance segmentation of cells
                                                    present in microscopy
                                                    images. Annotation of biomedical images in the lab can be scarce,
                                                    incomplete, and
                                                    inaccurate. This is of concern when supervised learning is used for
                                                    image analysis
                                                    as the discriminative power of a learning model might be compromised
                                                    in these
                                                    situations. To overcome the curse of poor labeling, our method
                                                    focuses on three
                                                    aspects to improve learning: i) we propose a loss function operating
                                                    in three
                                                    classes to facilitate separating adjacent cells and to drive the
                                                    optimizer to
                                                    properly classify underrepresented regions; ii) a contour-aware
                                                    weight map model is
                                                    introduced to strengthen contour detection while improving the
                                                    network
                                                    generalization capacity; and iii) we augment data by carefully
                                                    modulating local
                                                    intensities on edges shared by adjoining regions and to account for
                                                    possibly weak
                                                    signals on these edges. Generated probability maps are segmented
                                                    using different
                                                    methods, with the watershed based one generally offering the best
                                                    solutions,
                                                    specially in those regions where the prevalence of a single class is
                                                    not clear. The
                                                    combination of these contributions allows segmenting individual
                                                    cells on challenging
                                                    images. We demonstrate our methods in sparse and crowded cell
                                                    images, showing
                                                    improvements in the learning process for a fixed network
                                                    architecture. </i></p>
                                            <pre style="display: none;">
@incollection{guerrero2019weakly,
title={A Weakly Supervised Method for Instance Segmentation of Biological Cells},
author={Guerrero-Pe{\~n}a, Fidel A and Fernandez, Pedro D Marrero
and Ren, Tsang Ing and Cunha, Alexandre},
booktitle={Domain Adaptation and Representation Transfer and Medical
  Image Learning with Less Labels and Imperfect Data},
pages={216--224},
year={2019},
publisher={Springer}
}
            </pre>
                                        </div>
                                    </div>
                                </td>
                            </tr>
                            </td>
                            </tr>




                            <!-- item 4 -->

                            <tr>
                                <td class="research-table-image-td">
                                    <img src="assets/papers/burst.png" class="research-table-image" alt="ContextLocNet">
                                </td>

                                <!-- description -->
                                <td class="research-table-desc-td">
                                    <h3 class="font-medium">Burst ranking for blind multi-image deblurring
                                    </h3>
                                    <div class="research-table-desc-authors">
                                        Fidel A. Guerrero PeÃ±a, Pedro D. Marrero FernÃ¡ndez, Tsang Ing Ren, Jorge JG and
                                        Nishihara, Ricardo; <em> IEEE Transactions on Image Processing </em> 2019.
                                    </div>
                                    <div class="project-links-home" id="burst_bib">
                                        [<a href="https://doi.org/10.1109/TIP.2019.2936073">paper</a>] &nbsp;
                                        [<a href="https://arxiv.org/abs/1810.12121">arxiv</a>] &nbsp;
                                        [<a href="https://github.com/pedrodiamel/">code</a>] &nbsp;
                                        [<a href="javascript:toggleblock('burst_abs')">abstract</a>] &nbsp;
                                        [<a href="javascript:togglebib('burst_bib')" shape="rect"
                                            class="togglebib">bibtex</a>] &nbsp;

                                        <div class="research-table-desc-abstract">
                                            <p align="justify"> <i id="burst_abs" style="display: none;"> We propose a
                                                    new incremental
                                                    aggregation algorithm for multi-image deblurring with automatic
                                                    image selection. The
                                                    primary motivation is that current bursts deblurring methods do not
                                                    handle well
                                                    situations in which misalignment or out-of-context frames are
                                                    present in the burst.
                                                    These real-life situations result in poor reconstructions or manual
                                                    selection of the
                                                    images that will be used to deblur. Automatically selecting best
                                                    frames within the
                                                    burst
                                                    to improve the base reconstruction is challenging because the amount
                                                    of possible
                                                    images
                                                    fusions is equal to the power set cardinal. Here, we approach the
                                                    multi-image
                                                    deblurring
                                                    problem as a two steps process. First, we successfully learn a
                                                    comparison function
                                                    to
                                                    rank a burst of images using a deep convolutional neural network.
                                                    Then, an
                                                    incremental
                                                    Fourier burst accumulation with a reconstruction degradation
                                                    mechanism is applied
                                                    fusing
                                                    only less blurred images that are sufficient to maximize the
                                                    reconstruction quality.
                                                    Experiments with the proposed algorithm have shown superior results
                                                    when compared to
                                                    other similar approaches, outperforming other methods described in
                                                    the literature in
                                                    previously described situations. We validate our findings on several
                                                    synthetic and
                                                    real
                                                    datasets. </i></p>
                                            <pre style="display: none;">
@article{pena2019burst,
title={Burst Ranking for Blind Multi-Image Deblurring},
author={Pe{\~n}a, Fidel Alejandro Guerrero and Fern{\'a}ndez, Pedro Diamel Marrero
and Ren, Tsang Ing and Leandro, Jorge de Jesus Gomes and Nishihara, Ricardo Massahiro},
journal={IEEE Transactions on Image Processing},
volume={29},
pages={947--958},
year={2019},
publisher={IEEE}
}
            </pre>
                                        </div>
                                    </div>
                                </td>
                            </tr>
                            </td>
                            </tr>




                            <!-- item 3  -->

                            <tr>
                                <td class="research-table-image-td">
                                    <img src="assets/papers/feratt.gif" class="research-table-image"
                                        alt="ContextLocNet">
                                </td>

                                <!-- description -->
                                <td class="research-table-desc-td">
                                    <h3 class="font-medium">FERAtt: Facial Expression Recognition with Attention Net
                                    </h3>
                                    <div class="research-table-desc-authors">
                                        Pedro D. Marrero FernÃ¡ndez, Fidel A. Guerrero PeÃ±a, Tsang Ing Ren, Alexandre
                                        Cunha; <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
                                            Workshops </em> 2019.
                                    </div>
                                    <div class="project-links-home" id="attfer_bib">
                                        [<a
                                            href="http://openaccess.thecvf.com/content_CVPRW_2019/html/MBCCV/Fernandez_FERAtt_Facial_Expression_Recognition_With_Attention_Net_CVPRW_2019_paper.html">paper</a>]
                                        &nbsp;
                                        [<a href="https://arxiv.org/abs/1902.03284">arxiv</a>] &nbsp;
                                        [<a href="https://github.com/pedrodiamel/ferattention">code</a>] &nbsp;
                                        [<a href="javascript:toggleblock('attfer_abs')">abstract</a>] &nbsp;
                                        [<a href="javascript:togglebib('attfer_bib')" shape="rect"
                                            class="togglebib">bibtex</a>] &nbsp;

                                        <div class="research-table-desc-abstract">
                                            <p align="justify"> <i id="attfer_abs" style="display: none;"> We present a
                                                    new end-to-end
                                                    network
                                                    architecture for facial expression recognition with an attention
                                                    model. It focuses
                                                    attention in
                                                    the human face and uses a Gaussian space representation for
                                                    expression recognition.
                                                    We
                                                    devise
                                                    this architecture based on two fundamental complementary components:
                                                    (1) facial
                                                    image
                                                    correction
                                                    and attention and (2) facial expression representation and
                                                    classification. The first
                                                    component
                                                    uses an encoder-decoder style network and a convolutional feature
                                                    extractor that are
                                                    pixel-wise
                                                    multiplied to obtain a feature attention map. The second component
                                                    is responsible
                                                    for
                                                    obtaining
                                                    an embedded representation and classification of the facial
                                                    expression. We propose a
                                                    loss
                                                    function that creates a Gaussian structure on the representation
                                                    space. To
                                                    demonstrate
                                                    the
                                                    proposed method, we create two larger and more comprehensive
                                                    synthetic datasets
                                                    using
                                                    the
                                                    traditional BU3DFE and CK+ facial datasets. We compared results with
                                                    the
                                                    PreActResNet18
                                                    baseline. Our experiments on these datasets have shown the
                                                    superiority of our
                                                    approach
                                                    in
                                                    recognizing facial expressions. </i></p>
                                            <pre style="display: none;">
InProceedings{Fernandez_2019_CVPR_Workshops,
author = {Marrero Fernandez, Pedro D. and Guerrero Pena, Fidel A.
          and Ing Ren, Tsang and Cunha, Alexandre},
title = {FERAtt: Facial Expression Recognition With Attention Net},
booktitle = {The IEEE Conference on Computer Vision
          and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2019}
}
            </pre>
                                        </div>
                                    </div>
                                </td>
                            </tr>
                            </td>
                            </tr>



                            <!-- item 2 -->
                            <tr>
                                <td class="research-table-image-td">
                                    <img src="https://github.com/pedrodiamel/colorchecker-detection/raw/master/rec/mcc.gif"
                                        class="research-table-image" alt="ContextLocNet">
                                </td>

                                <!-- description -->
                                <td class="research-table-desc-td">
                                    <h3 class="font-medium">Fast and Robust Multiple ColorChecker Detection using Deep
                                        Convolutional
                                        Neural Networks
                                    </h3>
                                    <div class="research-table-desc-authors">
                                        Pedro D. Marrero FernÃ¡ndez, Fidel A. Guerrero PeÃ±a, Tsang Ing Ren, Jorge J.G.
                                        Leandro; <em>Image and Vision Computing</em> 2019.
                                    </div>
                                    <div class="project-links-home" id="colorchecker_detection">
                                        [<a href="https://doi.org/10.1016/j.imavis.2018.11.001">paper</a>] &nbsp;
                                        [<a href="https://arxiv.org/abs/1810.08639">arxiv</a>] &nbsp;
                                        [<a href="https://github.com/pedrodiamel/colorchecker-detection">code</a>]
                                        &nbsp;
                                        [<a href="javascript:toggleblock('colorchecker_detection_abs')">abstract</a>]
                                        &nbsp;
                                        [<a href="javascript:togglebib('colorchecker_detection')" shape="rect"
                                            class="togglebib">bibtex</a>]
                                        &nbsp;

                                        <div class="research-table-desc-abstract">
                                            <p align="justify"> <i id="colorchecker_detection_abs"
                                                    style="display: none;"> ColorCheckers
                                                    are
                                                    reference standards that professional photographers and filmmakers
                                                    use to ensure
                                                    predictable
                                                    results under every lighting condition. The objective of this work
                                                    is to propose a
                                                    new
                                                    fast
                                                    and robust method for automatic ColorChecker detection. The process
                                                    is divided into
                                                    two
                                                    steps: (1) ColorCheckers localization and (2) ColorChecker patches
                                                    recognition. For
                                                    the
                                                    ColorChecker localization, we trained a detection convolutional
                                                    neural network using
                                                    synthetic images. The synthetic images are created with the 3D
                                                    models of the
                                                    ColorChecker
                                                    and different background images. The output of the neural networks
                                                    are the bounding
                                                    box
                                                    of
                                                    each possible ColorChecker candidates in the input image. Each
                                                    bounding box defines
                                                    a
                                                    cropped image which is evaluated by a recognition system, and each
                                                    image is
                                                    canonized
                                                    with
                                                    regards to color and dimensions. Subsequently, all possible color
                                                    patches are
                                                    extracted
                                                    and
                                                    grouped with respect to the center's distance. Each group is
                                                    evaluated as a
                                                    candidate
                                                    for a
                                                    ColorChecker part, and its position in the scene is estimated.
                                                    Finally, a cost
                                                    function
                                                    is
                                                    applied to evaluate the accuracy of the estimation. The method is
                                                    tested using real
                                                    and
                                                    synthetic images. The proposed method is fast, robust to overlaps
                                                    and invariant to
                                                    affine
                                                    projections. The algorithm also performs well in case of multiple
                                                    ColorCheckers
                                                    detection.
                                                </i></p>
                                            <pre style="display: none;">
@article{MARREROFERNANDEZ2018,
  title = "Fast and Robust Multiple ColorChecker Detection
           using Deep Convolutional Neural Networks",
  journal = "Image and Vision Computing",
  year = "2018",
  issn = "0262-8856",
  author = "Pedro D. Marrero FernÃ¡ndez and Fidel A. Guerrero PeÃ±a
            and Tsang Ing Ren and Jorge J.G. Leandro",
}
            </pre>
                                        </div>
                                    </div>
                                </td>
                            </tr>
                            </td>
                            </tr>


                            <!-- item 1 -->

                            <tr>
                                <td class="research-table-image-td">
                                    <img src="assets/papers/mwlcs.png" class="research-table-image" alt="ContextLocNet">
                                </td>

                                <!-- description -->
                                <td class="research-table-desc-td">
                                    <h3 class="font-medium">Multiclass Weighted Loss for Instance Segmentation of
                                        Cluttered Cells
                                    </h3>
                                    <div class="research-table-desc-authors">
                                        Fidel A Guerrero-Pena, Pedro D Marrero Fernandez, Tsang Ing Ren, Mary Yui, Ellen
                                        Rothenberg, Alexandre Cunha; <em>IEEE International Conference on Image
                                            Processing</em> 2018.
                                    </div>
                                    <div class="project-links-home" id="mwlis">
                                        [<a href="https://doi.org/10.1109/ICIP.2018.8451187">paper</a>] &nbsp;
                                        [<a href="https://arxiv.org/abs/1802.07465">arxiv</a>] &nbsp;
                                        [<a href=" ">code</a>] &nbsp;
                                        [<a href="javascript:toggleblock('mwlis_abs')">abstract</a>] &nbsp;
                                        [<a href="javascript:togglebib('mwlis')" shape="rect"
                                            class="togglebib">bibtex</a>] &nbsp;

                                        <div class="research-table-desc-abstract">
                                            <p align="justify"> <i id="mwlis_abs" style="display: none;"> We propose a
                                                    new multiclass
                                                    weighted
                                                    loss function for instance segmentation of cluttered cells. We are
                                                    primarily
                                                    motivated
                                                    by
                                                    the need of developmental biologists to quantify and model the
                                                    behavior of blood
                                                    Tâ€“cells
                                                    which might help us in understanding their regulation mechanisms and
                                                    ultimately help
                                                    researchers in their quest for developing an effective immunotherapy
                                                    cancer
                                                    treatment.
                                                    Segmenting individual touching cells in cluttered regions is
                                                    challenging as the
                                                    feature
                                                    distribution on shared borders and cell foreground are similar thus
                                                    difficulting
                                                    discriminating pixels into proper classes. We present two novel
                                                    weight maps applied
                                                    to
                                                    the
                                                    weighted cross entropy loss function which take into account both
                                                    class imbalance
                                                    and
                                                    cell
                                                    geometry. Binary ground truth training data is augmented so the
                                                    learning model can
                                                    handle
                                                    not only foreground and background but also a third touching class.
                                                    This framework
                                                    allows
                                                    training using U-Net. Experiments with our formulations have shown
                                                    superior results
                                                    when
                                                    compared to other similar schemes, outperforming binary class models
                                                    with
                                                    significant
                                                    improvement of boundary adequacy and instance detection. We validate
                                                    our results on
                                                    manually
                                                    annotated microscope images of Tâ€“cells. </i></p>
                                            <pre style="display: none;">
@inproceedings{guerrero2018multiclass,
title={Multiclass weighted loss for instance segmentation
  of cluttered cells},
author={Guerrero-Pena, Fidel A and Fernandez, Pedro D Marrero and
  Ren, Tsang Ing and Yui, Mary and Rothenberg, Ellen and Cunha, Alexandre},
booktitle={2018 25th IEEE International Conference on
  Image Processing (ICIP)},
pages={2451--2455},
year={2018},
organization={IEEE}
}
            </pre>
                                        </div>
                                    </div>
                                </td>
                            </tr>
                            </td>
                            </tr>



                            <!-- item 0 -->



                        </table>

                    </section>






                    <p class="section-split">
                        ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                    </p>

                    <section id="" class="setion-content">
                        <div id="contact" class="content-space"></div>
                        <div class="">
                            <h1 class="content-container">
                                <span class="material-symbols-outlined content-span"> mail </span>
                                Contact
                            </h1><br>
                            <!-- <p>For product support or questions please: </p> -->
                            <a href="mailto:pedrodiamel@gmail.com" class="content-email">pedrodiamel@gmail.com</a>
                        </div><br>

                    </section>

                    <section class="section-terms">
                        <a class="section-terms-text" href="#"> Terms of Service </a>
                        <a class="section-terms-text" href="#"> Privacy Policy </a>
                    </section>

                </div>

            </div>

        </main>

    </div>

</body>

<script src="js/scripts.js"></script>

</html>